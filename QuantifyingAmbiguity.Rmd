---
title: "Quantifying Ambiguity"
author: "KBC"
date: "3/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(readr) # for read_file
library(purrr) # for map_chr
library(tidyr) # for separate
library(stringr)   
library(qdap) # for checking spelling
library(hunspell) # for checking spelling
```

## Introduction

Quantitative analyses of aspects of how _clean_ the data is, and of how _difficult_ it is likely to be from the perspective of understanding its meaning.

Associated code:

/Users/transfer/Documents/Scripts-new/ambiguity.measures.pl

```{r read.in.data.old, eval = FALSE}
#data.mimic <- "My dog is a very bad dog. I love her terribly.  She loves me, too. She is very jealous of me. She likes to go for walks with me."

# PHYSICIAN NOTES
data.mimic <- read_file("/Users/transfer/Dropbox/a-m/Corpora/MIMIC2/mimic2_500K.txt")
data.mimic <- data_frame(line = 1, text = data.mimic)
counts.mimic <- data.mimic %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.mimic)), counts.mimic$n)
counts.mimic

# NURSING NOTES
path.mimic.nursing <- "/Users/transfer/Dropbox/a-m/Corpora/MIMIC2/nursing/"
data.mimic.nursing <- list.files(path = path.mimic.nursing, pattern = "") %>% 
        map_chr(~ read_file(paste(path.mimic.nursing, ., sep = ""))) %>% 
        data_frame(line = 1, text = .)
counts.mimic.nursing <- data.mimic.nursing %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.mimic.nursing)), counts.mimic.nursing$n)
counts.mimic.nursing

# RADIOLOGY REPORTS
# NURSING NOTES
path.mimic.radiology <- "/Users/transfer/Dropbox/a-m/Corpora/MIMIC2/radiology/kev_clinical_radiology.txt"
#data.mimic.radiology <- list.files(path = path.mimic.radiology, pattern = "") %>% 
        #map_chr(~ read_file(paste(path.mimic.radiology, ., sep = ""))) %>% 
#        data_frame(line = 1, text = .)

data.mimic.radiology <- read_file(path.mimic.radiology)
data.mimic.radiology <- data_frame(line = 1, text = data.mimic.radiology)
counts.mimic.radiology <- data.mimic.radiology %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.mimic.radiology)), counts.mimic.radiology$n)
counts.mimic.radiology


# CRAFT CORPUS
path.craft <- "/Users/transfer/Dropbox/a-m/Corpora/craft-2.0/articles/txt/"
data.craft <- list.files(path = path.craft, pattern = "") %>% 
        map_chr(~ read_file(paste(path.craft, ., sep = ""))) %>% 
        data_frame(line = 1, text = .)
counts.craft <- data.craft %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.craft)), counts.craft$n)
counts.craft


```

## Read in data
```{r}
read.in.corpus <- function(my.path) {
my.data <- list.files(path = my.path, pattern = "") %>% 
        map_chr(~ read_file(paste(my.path, ., sep = ""))) %>% 
        data_frame(line = 1, text = .)
return(my.data)
}
data.raw <- read.in.corpus("/Users/transfer/Dropbox/a-m/Corpora/craft-2.0/articles/txt/")
typeof(data.raw)
head(data.raw)
```

## Produce counts
```{r}
# I *think* the expected input is a data frame
get.counts <- function(data.raw) {
counts <- data.raw %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

return(counts)
}
counts <- get.counts(data.raw)
head(counts)
```


## What are the most common prepositions?

```{r, eval = FALSE}
counts.mimic
counts.mimic.nursing
counts.mimic.radiology
counts.craft
```

...so, it looks like the most common preposition is _of_.

## Looking at _of_ in the various corpora

We want to know what kinds of things can take _of_ as a prepositional phrase modifier, and what kinds of things can be the nouns in those prepositional phrases.  We could do that with Sketch Engine, but let's try just looking at c ollocations and adjacent words and see how far that gets us.  If nothing else, it might give us some hints about what kinds of things to focus on in Sketch Engine.  In the best-case scenario, it will give us enough things to make clear what kinds of relationships we should be looking at in the qualitative analysis.

So: let's start by getting the common trigrams.  That gets us the thing being modified (modulo a lot of noise from cases where it's actually attached to a verb that's too far in front of it for us to see it) and the thing in the prepositional phrase.

```{r counting.trigrams.mimic, eval = FALSE}
data.mimic %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```

In the discharge summaries, the most frequent use of _of_ is in temporal expressions: _date of birth_, _day of life_ (the patients are pediatric), _time of discharge_, _day of admission_, _months of age_, and _day of discharge_.

```{r counting.trigrams.nursing, eval = FALSE}
data.mimic.nursing %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```

In contrast, _of_ has much more diverse functions in the nursing notes---no single function dominates, as temporal expressions do in the physicians' discharge notes.  Document sections (function = structuring the discourse): _plan of care_, _review of systems_, a temporal expression _day of life_, modifying a transparent noun (not listed in Merriam-Webster!) _amounts of thick_, _amts of thick_...

```{r counting.trigrams.radiology, eval = FALSE}
data.mimic.radiology %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```

Need to figure out the radiology ones...

In CRAFT, on the other hand, the predominant function is that of indicating an argument of a preceding nominalization (Merriam-Webster's 9(b)): _expression of the_, _analysis of the_, _disruption of the_.

```{r counting.trigrams.craft, eval = FALSE}
data.craft %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```

## This is where you look for the rate of spelling errors...

```{r hapax.analysis}

# input: a tibble with two columns: word and n, where n is the count for that word
estimate.rate.of.spelling.errors <- function(counts) {
counts
  
hapax <- counts %>% filter(n == 1)
head(hapax)
print(paste("Words in corpus that only occur once:", nrow(hapax)))
hapax.filtered <- hapax %>% filter(str_detect(word, "^[\\d\\.\\-\\,]+$") == FALSE)
print(paste("...after filtering out many numbers:", nrow(hapax.filtered)))
hapax.filtered

hapax.nonumbers <- hapax %>% filter(str_detect(word, "\\d") == FALSE)
print(paste("...after filtering out all words containing numbers:", nrow(hapax.nonumbers)))
hapax.nonumbers

hapax.with.punctuation <- hapax.nonumbers %>% filter(str_detect(word, "[\\.\\,\\:\\-\\;]"))
print(paste("How many of those contain punctuation?", nrow(hapax.with.punctuation)))
hapax.with.punctuation

hapax.without.punctuation <- hapax.nonumbers %>% filter(str_detect(word, "[\\.\\,\\:\\-\\;]") == FALSE)
print(paste("After removing punctuation, too:", nrow(hapax.without.punctuation)))

#hunspell_check(hapax.without.punctuation$word, dict = dictionary("en_US"))
hapax.without.punctuation.spellchecks <- hunspell_check(hapax.without.punctuation$word, dict = dictionary("en_US"))

#hapax.without.punctuation.spellchecks <- as_tibble(hapax.without.punctuation.spellchecks)

# TODO do this again, but with mutate()
hapax.without.punctuation <- as_tibble(hapax.without.punctuation)
hapax.without.punctuation <- mutate(hapax.without.punctuation, spellcheck = hapax.without.punctuation.spellchecks)

hapax.without.punctuation

hapax.spelling.ok <- hapax.without.punctuation %>% filter(spellcheck == TRUE)
hapax.spelling.ok
hapax.spelling.bad <- hapax.without.punctuation %>% filter(spellcheck == FALSE)
hapax.spelling.bad

# the estimated rate of spelling errors is the number of words that only occur once AND don't contain numbers or punctuation AND fail a spelling check, divided by the total count of word tokens, i.e. words in the document collection. So:

total.tokens <- sum(counts$n)
estimated.frequency.of.errors <- (nrow(hapax.spelling.bad) / total.tokens) * 100
print(paste("Estimated percentage of spelling errors in this data:", estimated.frequency.of.errors))

#hapax.without.punctuation %>% mutate(spellcheck.result, hunspell_check(hapax.without.punctuation$word, dict = dictionary("en_US")))

#hapax.without.punctuation.spellchecks
#hapax.spelling.errors <- hapax.without.punctuation.spellchecks %>% filter(value == FALSE)
#print(paste("Number of errors after removing all words containing numbers or punctuation:"), length(hapax.spelling.errors)))

} # close function definition estimate.rate.of.spelling.errors()
#estimate.rate.of.spelling.errors(counts.craft)
#estimate.rate.of.spelling.errors(counts.mimic)
#estimate.rate.of.spelling.errors(counts.mimic.radiology)
#estimate.rate.of.spelling.errors(counts.mimic.nursing)
estimate.rate.of.spelling.errors(counts)
```


