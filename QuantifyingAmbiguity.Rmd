---
title: "Quantifying Ambiguity"
author: "KBC"
date: "3/28/2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(readr) # for read_file
library(purrr) # for map_chr
library(tidyr) # for separate
library(stringr)   
library(qdap) # for checking spelling
library(hunspell) # for checking spelling
```

## Introduction

Quantitative analyses of aspects of how _clean_ the data is, and of how _difficult_ it is likely to be from the perspective of understanding its meaning.

### Associated code

The data that is being analyzed here gets generated by:

/Users/transfer/Documents/Scripts-new/ambiguity.measures.pl

_Actually, I've refactored the whole thing to no longer rely on the Perl script.  That makes this a lot easier to keep track of, but it means that the whole thing will have to be run locally.

## Read in data

_Input:_ String containing the path to a corpus

_Output:_ A tibble with one document per row.
```{r}
# Note that this function reads in an entire corpus, which means that the resulting data can only be used to analyze the entire corpus.  That's good for a lot of things, but for some other things, we will need to process one file at a time.
read.in.corpus <- function(my.path) {
my.data <- list.files(path = my.path, pattern = "txt$") %>% 
        map_chr(~ read_file(paste(my.path, ., sep = ""))) %>% 
        data_frame(line = 1, text = .)
return(my.data)
}
data.raw <- read.in.corpus("/Users/transfer/Dropbox/a-m/Corpora/craft-2.0/articles/txt/")
#typeof(data.raw)
head(data.raw)
```

## Produce counts
```{r}
# I *think* the expected input is a data frame
get.counts <- function(data.raw) {
counts <- data.raw %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

return(counts)
}
counts <- get.counts(data.raw)
head(counts)
```

One useful thing to do with the raw frequencies (counts) is to check for words in your document templates (such as fields for the patient name or for the History and Physical) that you might want me to remove from the analysis.

## Basic distributional characteristic: frequency/rank relationship

For any sufficiently large sample of language, relationship between frequency and frequency-ordered rank should be Zipfian.  

The reverse does not _necessarily_ hold--seeing a Zipfian relationship does _not_ necessarily imply that the sample size is "sufficiently large."

```{r}
plot.frequency.rank.relationship <- function(counts) {
  plot(c(1:nrow(counts)), counts$n,
       main = "Raw frequencies and ranks without ties")
}
plot.frequency.rank.relationship(counts)
```

## This is where you look for the rate of spelling errors...

If "data quality" means something like "how much garbage is there in my data?", then one estimate of that would be the incidence of spelling errors.  Here we estimate that incidence by:

1. Finding the words that only occur once
2. Removing any words that contain numbers (often lab values and the like) or punctuation (similar)
3. Run the remaining words through a spell checker

#### Why only look at words that only occur once?

By looking at words that only occur once, we make sure that local abbreviations, slang, technical terms, etc. do not get mistakenly flagged by the spell checker.  We then eliminate things that look like lab values, physiological measurements, etc. because those are not spelling errors, as such (although they do raise separate issues).

_John, I can make this print out for you without the code for the purpose of putting a presentation together, or I can make it print out the figures into individual files to plug into a PowerPoint presentation, or you can edit the code out manually in Word--whatever your prefer._

```{r hapax.analysis}

# input: a tibble with two columns: word and n, where n is the count for that word
estimate.rate.of.spelling.errors <- function(counts) {
counts
  
hapax <- counts %>% filter(n == 1)
head(hapax)
print(paste("Words in corpus that only occur once:", nrow(hapax)))
hapax.filtered <- hapax %>% filter(str_detect(word, "^[\\d\\.\\-\\,]+$") == FALSE)
print(paste("...after filtering out many numbers:", nrow(hapax.filtered)))
hapax.filtered

hapax.nonumbers <- hapax %>% filter(str_detect(word, "\\d") == FALSE)
print(paste("...after filtering out all words containing numbers:", nrow(hapax.nonumbers)))
hapax.nonumbers

hapax.with.punctuation <- hapax.nonumbers %>% filter(str_detect(word, "[\\.\\,\\:\\-\\;]"))
print(paste("How many of those contain punctuation?", nrow(hapax.with.punctuation)))
hapax.with.punctuation

hapax.without.punctuation <- hapax.nonumbers %>% filter(str_detect(word, "[\\.\\,\\:\\-\\;]") == FALSE)
print(paste("After removing punctuation, too:", nrow(hapax.without.punctuation)))

#hunspell_check(hapax.without.punctuation$word, dict = dictionary("en_US"))
hapax.without.punctuation.spellchecks <- hunspell_check(hapax.without.punctuation$word, dict = dictionary("en_US"))

#hapax.without.punctuation.spellchecks <- as_tibble(hapax.without.punctuation.spellchecks)

# TODO do this again, but with mutate()
hapax.without.punctuation <- as_tibble(hapax.without.punctuation)
hapax.without.punctuation <- mutate(hapax.without.punctuation, spellcheck = hapax.without.punctuation.spellchecks)

hapax.without.punctuation

hapax.spelling.ok <- hapax.without.punctuation %>% filter(spellcheck == TRUE)
hapax.spelling.ok
hapax.spelling.bad <- hapax.without.punctuation %>% filter(spellcheck == FALSE)
hapax.spelling.bad

# the estimated rate of spelling errors is the number of words that only occur once AND don't contain numbers or punctuation AND fail a spelling check, divided by the total count of word tokens, i.e. words in the document collection. So:

total.tokens <- sum(counts$n)
estimated.frequency.of.errors <- (nrow(hapax.spelling.bad) / total.tokens) * 100
print(paste("Estimated percentage of spelling errors in this data:", estimated.frequency.of.errors))

return(estimated.frequency.of.errors)

#hapax.without.punctuation %>% mutate(spellcheck.result, hunspell_check(hapax.without.punctuation$word, dict = dictionary("en_US")))

#hapax.without.punctuation.spellchecks
#hapax.spelling.errors <- hapax.without.punctuation.spellchecks %>% filter(value == FALSE)
#print(paste("Number of errors after removing all words containing numbers or punctuation:"), length(hapax.spelling.errors)))

} # close function definition estimate.rate.of.spelling.errors()
#estimate.rate.of.spelling.errors(counts.craft)
#estimate.rate.of.spelling.errors(counts.mimic)
#estimate.rate.of.spelling.errors(counts.mimic.radiology)
#estimate.rate.of.spelling.errors(counts.mimic.nursing)
estimate.rate.of.spelling.errors(counts)
```

There are other things that we can do to estimate the quality of this data, as well.  For example, we've been looking at spelling errors so far, but the incidence of words that only occur once and are spelled _correctly_ is also an index of how difficult this data will be to work with.  You could think of that as the likelihood of having things in your real data that were not in the training data.  This is an important variable in understanding how well machine learning is likely to work for your data: the algorithm can only build a good model for things for which it has enough instances in the training data.  Another thing that we could take into account is how many of those things looked like lab values, physiological measurements, etc.--things like that can be normalized into usable features, which can _increase_ performance.  But, you get the flavor of how this part of the analysis works.

## How hard will it be to extract information from this?

Now that we have an estimate of how much garbage there is in the data, we would like to know how difficult it is likely to be to work with this data.
We will estimate _that_ by seeing how common some phenomena that we know cause problems in any language processing task are in our data.

```{r} 
quantify.negation <- function(data.tibble) {
negatives <- c("no", "not", "none", "denies", "nothing") # anything? any?

counts.negatives <- data.tibble %>% filter(word %in% negatives) %>%
  count(word, sort = TRUE) 
counts.negatives  
}
```

```{r}
quantify.prepositional.phrases <- function(data.tibble) {
prepositions <- c("of", "to", "by", "with", "from", "for")
counts.prepositions <- data.tibble %>% filter(word %in% prepositions) %>% count(word, sort = TRUE)
counts.prepositions
}
```

```{r}
quantify.anaphoric.reference <- function(data.tibble) {
anaphora <- c("i", "me", "my", "we", "us", "our", "you", "your", 
              "he", "him", "his", "she", "her", "they", "them", "their")
counts.anaphora <- data.tibble %>% filter(word %in% anaphora) %>% count(word, sort = TRUE)
counts.anaphora
}
```

```{r}
quantify.conjunctions <- function(data.tibble) {
conjunctions <- c("and", "or", "but")
counts.conjunctions <- data.tibble %>% filter(word %in% conjunctions) %>% count(word, sort = TRUE)
counts.conjunctions
# TODO: turn these counts into frequencies
# TODO: need to return something.  Maybe break this up into separate functions??
}
```


```{r}
# TODO: I could save some memory by doing this earlier, and then using the resulting object to get the counts, instead of running through unnest_tokens() twice.  
# Oh, that might not be true--it depends on whether or not we need individual files, e.g. per patient.  So far, we haven't.  But, for things like calculating type/token ratio, we will...

quantify.ambiguity <- function(data.raw) {
#Note that tibble is a kind of R data structure that is nice for doing quanitative analyses of textual data.
data.tibble <- data.raw %>%
  unnest_tokens(word, text) 

data.tibble
return(data.tibble)
} # close function definition quantify.ambiguity()
data.tibble <- quantify.ambiguity(data.raw)
quantify.negation(data.tibble)
quantify.anaphoric.reference(data.tibble)
quantify.conjunctions(data.tibble)
quantify.prepositional.phrases(data.tibble)
```

## Quantifying speculation

One indicator of how much you will be able to weight information mined from your text is how the writers express their confidence in their statements.  Quantifying confidence is complicated, but there is a lot known about how to do it.  For now, we will just estimate it by calculating the frequencies of some lexical cues to speculation, such as _rule out, possible,_ and the like.

```{r}
data.tibble <- data.raw %>%
  unnest_tokens(word, text) 

data.tibble

speculation.cues <- c("rule", "R/O", "possible", "eventual") # note that R/O may get broken up in earlier processing, "rule out" won't actually be found if you're just looking at single words, "could be" is a clear speculation cue, but requires that you look at two-word sequences--these issues can be coped with.  For the moment, we'll just look at the general concept.
data.tibble %>% filter(word %in% speculation.cues) %>% count(word, sort = TRUE)
```

### Now let's look at what kinds of functions some of these things play.  Prepositions can have many functions _and_ many meanings, so let's look at some of those.  Note that this will have to be a manual analysis.

```{r}
get.trigrams <- function(data.raw) {
  my.trigrams <- data.raw %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  #filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
  my.trigrams
  return(my.trigrams)
}
trigrams.all <- get.trigrams(data.raw)
trigrams.all
```

```{r}
filter.trigrams <- function(trigrams, string.to.match) {
  filtered.trigrams <- trigrams %>%
    filter(word2 == string.to.match) %>%
  count(word1, word2, word3, sort = TRUE)

  return(filtered.trigrams)
}
filtered.trigrams <- filter.trigrams(trigrams.all, "of")
filtered.trigrams
```




## Going through a corpus of clinical data

With that background, let's see how we can go through a single corpus of clinical data.

```{r}
data.raw <- read.in.corpus("/Users/transfer/Dropbox/a-m/Corpora/MIMIC2/md/")
```

```{r}
counts <- get.counts(data.raw)
head(counts)
```

```{r}
plot.frequency.rank.relationship(counts)
```

```{r}
estimate.rate.of.spelling.errors(counts)
```

```{r}
quantify.anaphoric.reference(data.tibble)
```

```{r}
quantify.conjunctions(data.tibble)
```

```{r}
quantify.prepositional.phrases(data.tibble)
```

```{r}
trigrams.all <- get.trigrams(data.raw)
```

```{r}
filtered.trigrams <- filter.trigrams(trigrams.all, "of")
filtered.trigrams
```

```{r}
filtered.trigrams <- filter.trigrams(trigrams.all, "to")
filtered.trigrams
```

```{r}
filtered.trigrams <- filter.trigrams(trigrams.all, "by")
filtered.trigrams
```

```{r}
filtered.trigrams <- filter.trigrams(trigrams.all, "for")
filtered.trigrams
```

```{r}
filtered.trigrams <- filter.trigrams(trigrams.all, "with")
filtered.trigrams
```

```{r}
filtered.trigrams <- filter.trigrams(trigrams.all, "from")
filtered.trigrams
```


_Need to pull out speculation into a function._





# JOHN, YOU CAN REMOVE ANYTHING AFTER THIS HEADING BEFORE SHARING

```{r read.in.data.old, eval = FALSE}
#data.mimic <- "My dog is a very bad dog. I love her terribly.  She loves me, too. She is very jealous of me. She likes to go for walks with me."

# PHYSICIAN NOTES
data.mimic <- read_file("/Users/transfer/Dropbox/a-m/Corpora/MIMIC2/mimic2_500K.txt")
data.mimic <- data_frame(line = 1, text = data.mimic)
counts.mimic <- data.mimic %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.mimic)), counts.mimic$n)
counts.mimic

# NURSING NOTES
path.mimic.nursing <- "/Users/transfer/Dropbox/a-m/Corpora/MIMIC2/nursing/"
data.mimic.nursing <- list.files(path = path.mimic.nursing, pattern = "") %>% 
        map_chr(~ read_file(paste(path.mimic.nursing, ., sep = ""))) %>% 
        data_frame(line = 1, text = .)
counts.mimic.nursing <- data.mimic.nursing %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.mimic.nursing)), counts.mimic.nursing$n)
counts.mimic.nursing

# RADIOLOGY REPORTS
# NURSING NOTES
path.mimic.radiology <- "/Users/transfer/Dropbox/a-m/Corpora/MIMIC2/radiology/kev_clinical_radiology.txt"
#data.mimic.radiology <- list.files(path = path.mimic.radiology, pattern = "") %>% 
        #map_chr(~ read_file(paste(path.mimic.radiology, ., sep = ""))) %>% 
#        data_frame(line = 1, text = .)

data.mimic.radiology <- read_file(path.mimic.radiology)
data.mimic.radiology <- data_frame(line = 1, text = data.mimic.radiology)
counts.mimic.radiology <- data.mimic.radiology %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.mimic.radiology)), counts.mimic.radiology$n)
counts.mimic.radiology


# CRAFT CORPUS
path.craft <- "/Users/transfer/Dropbox/a-m/Corpora/craft-2.0/articles/txt/"
data.craft <- list.files(path = path.craft, pattern = "") %>% 
        map_chr(~ read_file(paste(path.craft, ., sep = ""))) %>% 
        data_frame(line = 1, text = .)
counts.craft <- data.craft %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

plot(c(1:nrow(counts.craft)), counts.craft$n)
counts.craft


```




## What are the most common prepositions?

```{r, eval = FALSE}
counts.mimic
counts.mimic.nursing
counts.mimic.radiology
counts.craft
```

...so, it looks like the most common preposition is _of_.

## Looking at _of_ in the various corpora

We want to know what kinds of things can take _of_ as a prepositional phrase modifier, and what kinds of things can be the nouns in those prepositional phrases.  We could do that with Sketch Engine, but let's try just looking at c ollocations and adjacent words and see how far that gets us.  If nothing else, it might give us some hints about what kinds of things to focus on in Sketch Engine.  In the best-case scenario, it will give us enough things to make clear what kinds of relationships we should be looking at in the qualitative analysis.

So: let's start by getting the common trigrams.  That gets us the thing being modified (modulo a lot of noise from cases where it's actually attached to a verb that's too far in front of it for us to see it) and the thing in the prepositional phrase.

```{r counting.trigrams.mimic, eval = FALSE}
data.mimic %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```

In the discharge summaries, the most frequent use of _of_ is in temporal expressions: _date of birth_, _day of life_ (the patients are pediatric), _time of discharge_, _day of admission_, _months of age_, and _day of discharge_.

```{r counting.trigrams.nursing, eval = FALSE}
data.mimic.nursing %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```

In contrast, _of_ has much more diverse functions in the nursing notes---no single function dominates, as temporal expressions do in the physicians' discharge notes.  Document sections (function = structuring the discourse): _plan of care_, _review of systems_, a temporal expression _day of life_, modifying a transparent noun (not listed in Merriam-Webster!) _amounts of thick_, _amts of thick_...

```{r counting.trigrams.radiology, eval = FALSE}
data.mimic.radiology %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```

Need to figure out the radiology ones...

In CRAFT, on the other hand, the predominant function is that of indicating an argument of a preceding nominalization (Merriam-Webster's 9(b)): _expression of the_, _analysis of the_, _disruption of the_.

```{r counting.trigrams.craft, eval = FALSE}
data.craft %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word2 == "of") %>%
  count(word1, word2, word3, sort = TRUE)
```



